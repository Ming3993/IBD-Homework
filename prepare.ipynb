{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ming3993/IBD-Homework/blob/main/prepare.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epfrvqOy0XbK"
      },
      "source": [
        "# Install Java and Spark on Hadoop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pf3zOXQkRf0l",
        "outputId": "962cbd67-33b9-4851-8aec-4a317a32abdc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (185.125.190.83)] [Connected to cloud.r-pr\r                                                                               \rHit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (185.125.190.83)] [Connected to cloud.r-pr\r                                                                               \rHit:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n"
          ]
        }
      ],
      "source": [
        "# install java\n",
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "# install spark (change the version number if needed)\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.5.6/spark-3.5.6-bin-hadoop3.tgz\n",
        "# unzip the spark file to the current folder\n",
        "!tar xf spark-3.5.6-bin-hadoop3.tgz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Llc1FhGNQ3U5"
      },
      "outputs": [],
      "source": [
        "# set your spark folder to your system path environment.\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.6-bin-hadoop3\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfAmwnpt1G5p"
      },
      "source": [
        "# Create a SparkSession in Python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ExFt_N8-z2m",
        "outputId": "5464fa83-9656-4fba-d4fe-dd20a8b0ff11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: findspark in /usr/local/lib/python3.11/dist-packages (2.0.1)\n"
          ]
        }
      ],
      "source": [
        "# start pyspark\n",
        "!pip install findspark\n",
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install \"pymongo[srv]\""
      ],
      "metadata": {
        "id": "UDfmCL2eoxx0",
        "outputId": "4b9ed9f5-bb19-4998-e8d6-4b06ecdcb52f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymongo[srv]\n",
            "  Downloading pymongo-4.13.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
            "\u001b[33mWARNING: pymongo 4.13.2 does not provide the extra 'srv'\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting dnspython<3.0.0,>=1.16.0 (from pymongo[srv])\n",
            "  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Downloading dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pymongo-4.13.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dnspython, pymongo\n",
            "Successfully installed dnspython-2.7.0 pymongo-4.13.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pymongo.mongo_client import MongoClient\n",
        "from pymongo.server_api import ServerApi\n",
        "\n",
        "mongo_uri = \"mongodb+srv://Ming3993:311203@ming3993.mjnfwow.mongodb.net/?retryWrites=true&w=majority&appName=Ming3993\"\n",
        "\n",
        "# Create a new client and connect to the server\n",
        "client = MongoClient(mongo_uri, server_api=ServerApi('1'))\n",
        "\n",
        "# Send a ping to confirm a successful connection\n",
        "try:\n",
        "    client.admin.command('ping')\n",
        "    print(\"Pinged your deployment. You successfully connected to MongoDB!\")\n",
        "except Exception as e:\n",
        "    print(e)"
      ],
      "metadata": {
        "id": "tTpUHr4PqaA7",
        "outputId": "58a900cb-41bf-4330-8a57-ac7a253ea31c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pinged your deployment. You successfully connected to MongoDB!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "ZU-oJLNVQl45"
      },
      "outputs": [],
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Spark MongoDB Integration\") \\\n",
        "    .config(\"spark.mongodb.input.uri\", mongo_uri) \\\n",
        "    .config(\"spark.mongodb.output.uri\", mongo_uri) \\\n",
        "    .config(\n",
        "        \"spark.jars.packages\",\n",
        "        \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1,\"\n",
        "        \"graphframes:graphframes:0.8.4-spark3.5-s_2.12\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_core = spark.sparkContext.defaultParallelism"
      ],
      "metadata": {
        "id": "j279KWpBGycj"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chdGpJrATCDO"
      },
      "source": [
        "# Example 1: WordCount with Spark DataFrames and Spark RDDs\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91A6eRPJUl_0",
        "outputId": "fe9c5c11-2137-4823-8b69-6bdffe4d2923"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CSC14118'...\n",
            "remote: Enumerating objects: 17, done.\u001b[K\n",
            "remote: Counting objects: 100% (17/17), done.\u001b[K\n",
            "remote: Compressing objects: 100% (15/15), done.\u001b[K\n",
            "remote: Total 17 (delta 1), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (17/17), 818.44 KiB | 4.28 MiB/s, done.\n",
            "Resolving deltas: 100% (1/1), done.\n"
          ]
        }
      ],
      "source": [
        "# Load the data\n",
        "!git clone https://github.com/nnthaofit/CSC14118.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJj_K5siTgAx"
      },
      "source": [
        "### Spark DataFrame-based WordCount"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NACaHF44TG_L",
        "outputId": "16e835b7-c956-4f04-db60-912357980dbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------------+\n",
            "|value                       |\n",
            "+----------------------------+\n",
            "|ppap                        |\n",
            "|i have a pen                |\n",
            "|i have an apple             |\n",
            "|ah apple pen                |\n",
            "|i have a pen                |\n",
            "|i have a pineapple          |\n",
            "|ah pineapple pen            |\n",
            "|ppap pen pineapple apple pen|\n",
            "+----------------------------+\n",
            "\n",
            "+---------+-----+\n",
            "|     word|count|\n",
            "+---------+-----+\n",
            "|      pen|    6|\n",
            "|     have|    4|\n",
            "|        i|    4|\n",
            "|    apple|    3|\n",
            "|pineapple|    3|\n",
            "|        a|    3|\n",
            "|     ppap|    2|\n",
            "|       ah|    2|\n",
            "|       an|    1|\n",
            "+---------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "linesDF = spark.read.text(\"CSC14118/ppap.txt\")\n",
        "linesDF.show(linesDF.count(),truncate = False)\n",
        "\n",
        "from pyspark.sql import functions as f\n",
        "wordsDF = linesDF.withColumn(\"word\", f.explode(f.split(f.col(\"value\"), \" \")))\\\n",
        "    .groupBy(\"word\")\\\n",
        "    .count()\\\n",
        "    .sort(\"count\", ascending = False)\n",
        "wordsDF.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82hKvCdcTj6g"
      },
      "source": [
        "###RDD-based WordCount"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONOtAnLxSxYK",
        "outputId": "4c1c6725-7c6b-4fb8-fc09-ee4cbf72b7ed"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('pen', 6),\n",
              " ('have', 4),\n",
              " ('i', 4),\n",
              " ('apple', 3),\n",
              " ('pineapple', 3),\n",
              " ('a', 3),\n",
              " ('ppap', 2),\n",
              " ('ah', 2),\n",
              " ('an', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "linesRdd = spark.sparkContext.textFile(\"CSC14118/ppap.txt\")\n",
        "wordsRdd = linesRdd.flatMap(lambda line: line.split(\" \")) \\\n",
        "    .map(lambda word: (word, 1)) \\\n",
        "    .reduceByKey(lambda a, b: a + b)\\\n",
        "    .sortBy(lambda pair:-1*pair[1])\n",
        "wordsRdd.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 1: Data query with Spark DataFrame"
      ],
      "metadata": {
        "id": "eskNBM_J66UP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# clone the example data files from GitHub to Drive\n",
        "!git clone https://github.com/nnthaofit/CSC14118.git"
      ],
      "metadata": {
        "id": "T5YNLLU6656Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###0. Load the data file: movies.json"
      ],
      "metadata": {
        "id": "EbUxY_c-69xG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "movies_df = spark.read.json(\"movies.json\")\n",
        "\n",
        "movies_df.show(5, truncate=False)"
      ],
      "metadata": {
        "id": "ZMLl4NC4t6_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1a. Show the schema of DataFrame that stores the movies dataset."
      ],
      "metadata": {
        "id": "HNfJKss_7cG1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "movies_df.printSchema()"
      ],
      "metadata": {
        "id": "rJhoYcqQt8XV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1b. Show the number of distinct movies in the dataset"
      ],
      "metadata": {
        "id": "alyy1VZT7rve"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_distinct = movies_df.distinct().count()\n",
        "\n",
        "print(num_distinct)"
      ],
      "metadata": {
        "id": "u6Irt5YPt955"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Count the number of movies released during the years 2012 and 2015 (included)"
      ],
      "metadata": {
        "id": "lbRNx-QU73ZN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "movies_interval = movies_df.where(\"year <= 2015 AND year >= 2012\").count()\n",
        "\n",
        "movies_interval"
      ],
      "metadata": {
        "id": "AQFTKSdYt_9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Show the year in which the number of movies released is highest. One highest year is enough"
      ],
      "metadata": {
        "id": "Wm7NfqXp76Cy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_year_df = movies_df.groupBy(\"year\") \\\n",
        "    .count() \\\n",
        "    .sort(\"count\", ascending=False).show(1)"
      ],
      "metadata": {
        "id": "H6fj91IzuBcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Show the list of movies such that for each film, the number of actors/actresses is at least five, and the number of genres it belongs to is at most two genres."
      ],
      "metadata": {
        "id": "EpfIvw-h8Eep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "require_df = movies_df.withColumn(\"numCast\", f.size(f.col(\"cast\"))) \\\n",
        "    .withColumn(\"numGenre\", f.size(f.col(\"genres\"))) \\\n",
        "    .where(\"numGenre <= 2 and numCast >= 5\")\n",
        "require_df.select(\"title\", \"year\", \"cast\", \"genres\").show(5, truncate=False)\n",
        "require_df.drop(\"numGenre\", \"numCast\").show(5, False)"
      ],
      "metadata": {
        "id": "_TD58zTjuEzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Show the **movies** whose names are longest"
      ],
      "metadata": {
        "id": "TG1zzyVb8ND9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import desc, asc\n",
        "\n",
        "len_df = movies_df.withColumn(\"nameLen\", f.length(f.col(\"title\"))).orderBy(f.col(\"nameLen\").desc())\n",
        "\n",
        "max_num = len_df.first()[\"nameLen\"]\n",
        "\n",
        "require_df = len_df.where(f.col(\"nameLen\") == max_num).show(require_df.count(), False)"
      ],
      "metadata": {
        "id": "YsBt1ARluGJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Show the movies whose name contains the word “fighting” (case-insensitive)."
      ],
      "metadata": {
        "id": "h-UJvT_I8QHF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tmp_df = movies_df.withColumn(\"word\", f.split(f.lower(\"title\"), \" \"))\n",
        "tmp_df.where(f.array_contains(f.col(\"word\"), \"fighting\")).show()"
      ],
      "metadata": {
        "id": "fvTK-g-AuHnu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Show the list of distinct genres appearing in the dataset"
      ],
      "metadata": {
        "id": "sZwOdpkG8Wkl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "genre_df = movies_df.withColumn(\"genre\", f.explode(f.col(\"genres\"))).groupBy(\"genre\").count().select(\"genre\")\n",
        "genre_df.show()"
      ],
      "metadata": {
        "id": "e2Bkyz_DuI3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. List all movies in which the actor Harrison Ford has participated."
      ],
      "metadata": {
        "id": "zIS1IRFP8ZVt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parti_df = movies_df.where(f.array_contains(f.col(\"cast\"),\"harrison ford\"))\n",
        "parti_df.show(5,False)"
      ],
      "metadata": {
        "id": "Ooe3eFjDuJ8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. List all movies in which the actors/actresses whose names include the word “Lewis“ (case-insensitive) have participated."
      ],
      "metadata": {
        "id": "Gj6gxLvh8cXW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tmp_df = movies_df.withColumn(\"sinCast\", f.explode(f.col(\"cast\")))\n",
        "tmp_df = tmp_df.filter(f.lower(\"sinCast\").contains(\"lewis\")).drop(\"sinCast\").distinct()\n",
        "\n",
        "tmp_df.show()"
      ],
      "metadata": {
        "id": "GW8U4Yv1uLGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10. Show top five actors/actresses that have participated in most movies."
      ],
      "metadata": {
        "id": "s_EhHiV38fXV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "movies_df.withColumn(\"sinCast\", f.explode(f.col(\"cast\"))).groupBy(\"sinCast\").count().orderBy(f.col(\"count\").desc()).show(5,False)"
      ],
      "metadata": {
        "id": "8iHf9EAluOMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise 2: RDD-based mainpulation\n",
        "\n",
        "\n",
        "*   The data is already in one ore more RDDs.\n",
        "*   You must not convert RDD to DF or use pure Python code.\n"
      ],
      "metadata": {
        "id": "aHQb93lRHj3u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Consider a string s that includes only alphabetical letters and spaces. Check whether s is a palindrome (case-insensitive)."
      ],
      "metadata": {
        "id": "o1F4O-msHoDj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = spark.sparkContext.parallelize(\"\".lower(), num_core).filter(lambda x: x != ' ')\n",
        "rdd = rdd.repartition(1)\n",
        "\n",
        "rdd_cnt = spark.sparkContext.parallelize(range(0,rdd.count()), 1)\n",
        "\n",
        "rdd_a = rdd.zip(rdd_cnt)\n",
        "\n",
        "rdd_b = rdd_a.sortBy(lambda row: row[1] * -1)\n",
        "\n",
        "rdd_combine = rdd_a.zip(rdd_b)\n",
        "\n",
        "if (rdd_combine.filter(lambda row: row[0][0] != row[1][0]).count() == 0):\n",
        "  print(\"Palindrome\")\n",
        "else:\n",
        "  print(\"Not palindrome\")"
      ],
      "metadata": {
        "id": "sTSEfW6WuQ_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Consider a string s that includes only alphabetical letters and spaces. Check whether s is a pangram (case-insensitive)."
      ],
      "metadata": {
        "id": "J6QVU6UWHwYl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = spark.sparkContext.parallelize(\"The five boxing wizards jump quickly\".lower(), num_core).filter(lambda x: x != ' ')\n",
        "\n",
        "rdd = rdd.repartition(1)\n",
        "\n",
        "rdd.collect()\n",
        "\n",
        "if (rdd.distinct().count() == 26):\n",
        "  print(\"Pangram\")\n",
        "else:\n",
        "  print(\"Not pangram\")"
      ],
      "metadata": {
        "id": "KT8sz6JduSkB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a0e7130-73c9-44f2-f461-390ecafa51f0"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pangram\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise 3: Frequent patterns and association rules mining"
      ],
      "metadata": {
        "id": "QOoFk6Z2H13m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0. Load the data file: foodmart.csv\n",
        "\n",
        "\n",
        "*  A record is a tuple of binary values {0, 1}, each of which denotes the presence of an item (1: bought, 0: not bought).\n",
        "\n"
      ],
      "metadata": {
        "id": "uH3ozoTVH6J1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/nnthaofit/CSC14118.git\n",
        "df = spark.read.csv(\"CSC14118/foodmart.csv\", header=True, inferSchema = True)\n",
        "df.show()"
      ],
      "metadata": {
        "id": "4N9eo13QH_Tt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e31a2809-4365-44b6-aec3-46b1c80dcac4"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'CSC14118' already exists and is not an empty directory.\n",
            "+------------+---------+-------+--------------+------+---------+----+-------+-------+------------+-----------------+------+------+-----+---------+---------------+-----+--------+------+-------------+------------------+-----------+-------+-----------+--------------+--------+----------+-----------+-----------+----+------+-----------+----------+----+-----------------+---------------+------------+-------------+----------+--------------+-----------------+---+---------+----------+--------------+--------+---------+---------+---+-----+-----+----------+----+----+---------+-------+------------+----+-------+-----------+--------+------------+-----------+-----+-------------+----------------+-----+----------------+-------+---------+------------+-------------+-------------+---------+--------+----+--------+------+------------+-------+---------+------+------------+----+----+----------+------+-------+----------------+-----+----------+----+--------------+-----+------------+----+---------+-------+----+------+\n",
            "|Acetominifen|Anchovies|Aspirin|Auto Magazines|Bagels|Batteries|Beer|Bologna|Candles|Canned Fruit|Canned Vegetables|Cereal|Cheese|Chips|Chocolate|Chocolate Candy|Clams|Cleaners|Coffee|Cold Remedies|Computer Magazines|Conditioner|Cookies|Cooking Oil|Cottage Cheese|Crackers|Deli Meats|Deli Salads|Deodorizers|Dips|Donuts|Dried Fruit|Dried Meat|Eggs|Fashion Magazines|Flavored Drinks|French Fries|Fresh Chicken|Fresh Fish|Frozen Chicken|Frozen Vegetables|Gum|Hamburger|Hard Candy|Home Magazines|Hot Dogs|Ibuprofen|Ice Cream|Jam|Jelly|Juice|Lightbulbs|Maps|Milk|Mouthwash|Muffins|Nasal Sprays|Nuts|Oysters|Pancake Mix|Pancakes|Paper Dishes|Paper Wipes|Pasta|Peanut Butter|Personal Hygiene|Pizza|Plastic Utensils|Popcorn|Popsicles|Pot Cleaners|Pot Scrubbers|Pots and Pans|Preserves|Pretzels|Rice|Sardines|Sauces|Screwdrivers|Shampoo|Shellfish|Shrimp|Sliced Bread|Soda|Soup|Sour Cream|Spices|Sponges|Sports Magazines|Sugar|Sunglasses|Tofu|Toilet Brushes|Tools|Toothbrushes|Tuna|TV Dinner|Waffles|Wine|Yogurt|\n",
            "+------------+---------+-------+--------------+------+---------+----+-------+-------+------------+-----------------+------+------+-----+---------+---------------+-----+--------+------+-------------+------------------+-----------+-------+-----------+--------------+--------+----------+-----------+-----------+----+------+-----------+----------+----+-----------------+---------------+------------+-------------+----------+--------------+-----------------+---+---------+----------+--------------+--------+---------+---------+---+-----+-----+----------+----+----+---------+-------+------------+----+-------+-----------+--------+------------+-----------+-----+-------------+----------------+-----+----------------+-------+---------+------------+-------------+-------------+---------+--------+----+--------+------+------------+-------+---------+------+------------+----+----+----------+------+-------+----------------+-----+----------+----+--------------+-----+------------+----+---------+-------+----+------+\n",
            "|           1|        0|      0|             0|     0|        0|   0|      0|      0|           0|                0|     0|     1|    0|        0|              0|    0|       0|     0|            0|                 0|          0|      0|          0|             0|       0|         0|          0|          0|   0|     0|          0|         0|   0|                0|              0|           0|            0|         0|             0|                0|  0|        0|         0|             1|       0|        0|        0|  0|    0|    0|         0|   0|   0|        0|      0|           0|   0|      0|          0|       0|           0|          0|    0|            0|               0|    0|               0|      0|        0|           0|            0|            0|        0|       0|   0|       0|     0|           0|      1|        0|     0|           0|   0|   0|         0|     0|      0|               0|    0|         0|   0|             0|    0|           0|   0|        0|      0|   0|     0|\n",
            "|           1|        0|      0|             0|     0|        0|   0|      0|      0|           0|                0|     0|     1|    0|        0|              0|    0|       0|     0|            0|                 0|          0|      0|          0|             0|       0|         0|          0|          0|   0|     0|          0|         0|   0|                0|              0|           0|            0|         0|             0|                0|  0|        0|         1|             0|       0|        0|        0|  0|    0|    0|         0|   0|   1|        0|      0|           0|   0|      0|          0|       0|           0|          0|    0|            0|               0|    0|               0|      0|        0|           0|            1|            0|        0|       0|   1|       0|     0|           0|      0|        0|     0|           0|   0|   0|         0|     0|      0|               0|    0|         0|   0|             0|    0|           0|   0|        0|      0|   0|     0|\n",
            "|           0|        0|      0|             0|     0|        0|   0|      0|      0|           0|                0|     0|     0|    0|        0|              0|    0|       0|     1|            0|                 0|          0|      0|          0|             0|       0|         0|          1|          0|   0|     0|          0|         0|   0|                0|              0|           0|            0|         0|             0|                0|  0|        0|         0|             0|       0|        0|        0|  0|    0|    0|         0|   0|   0|        0|      0|           0|   0|      0|          0|       0|           0|          0|    0|            0|               0|    0|               0|      0|        0|           0|            0|            0|        0|       0|   0|       0|     0|           0|      0|        0|     0|           0|   0|   0|         0|     0|      0|               0|    0|         0|   0|             0|    0|           0|   0|        0|      0|   0|     0|\n",
            "|           0|        0|      0|             0|     0|        0|   0|      0|      0|           0|                0|     0|     0|    0|        0|              0|    0|       0|     0|            0|                 0|          0|      0|          0|             0|       0|         0|          0|          0|   0|     0|          0|         0|   1|                0|              0|           0|            0|         0|             0|                0|  1|        0|         0|             0|       0|        0|        0|  0|    0|    0|         0|   0|   1|        0|      0|           0|   0|      0|          0|       0|           0|          0|    0|            0|               0|    0|               0|      0|        0|           0|            0|            0|        0|       0|   0|       0|     0|           0|      0|        0|     0|           0|   0|   1|         0|     0|      0|               0|    0|         0|   0|             0|    0|           0|   0|        0|      0|   0|     0|\n",
            "|           0|        0|      0|             0|     0|        0|   0|      0|      0|           0|                0|     0|     1|    0|        0|              0|    0|       0|     0|            0|                 0|          0|      0|          0|             0|       0|         0|          0|          0|   0|     0|          1|         0|   0|                0|              0|           0|            0|         0|             1|                0|  0|        0|         0|             0|       0|        0|        0|  0|    0|    0|         0|   0|   0|        0|      0|           0|   0|      0|          0|       0|           0|          0|    0|            0|               0|    0|               1|      0|        0|           0|            0|            0|        0|       0|   0|       0|     0|           0|      0|        0|     0|           0|   0|   0|         0|     0|      0|               0|    0|         0|   0|             0|    0|           0|   0|        0|      0|   0|     0|\n",
            "|           0|        0|      0|             0|     0|        0|   0|      0|      0|           0|                0|     0|     0|    0|        0|              0|    0|       0|     0|            0|                 0|          0|      0|          0|             0|       0|         0|          0|          0|   0|     0|          0|         0|   0|                0|              0|           0|            0|         0|             0|                0|  0|        0|         0|             0|       0|        0|        0|  0|    0|    0|         0|   0|   0|        0|      0|           0|   0|      0|          0|       0|           0|          0|    0|            0|               0|    0|               0|      0|        0|           0|            0|            0|        0|       0|   0|       0|     0|           0|      1|        0|     0|           0|   0|   0|         0|     0|      0|               0|    0|         0|   0|             0|    0|           0|   0|        0|      0|   0|     0|\n",
            "|           0|        0|      0|             0|     0|        0|   0|      0|      0|           0|                0|     0|     0|    0|        0|              0|    0|       0|     0|            0|                 0|          0|      0|          0|             0|       0|         0|          0|          0|   0|     0|          0|         0|   0|                0|              0|           0|            0|         0|             0|                0|  0|        0|         0|             0|       0|        0|        0|  0|    0|    0|         0|   0|   1|        0|      0|           0|   0|      0|          0|       0|           0|          1|    0|            0|               0|    0|               0|      0|        0|           0|            0|            0|        0|       0|   0|       0|     0|           0|      0|        0|     0|           0|   0|   0|         0|     0|      0|               0|    0|         0|   0|             0|    0|           0|   0|        0|      1|   0|     0|\n",
            "|           0|        0|      0|             0|     0|        0|   0|      0|      0|           0|                0|     0|     0|    0|        0|              0|    0|       0|     0|            0|                 0|          0|      0|          0|             0|       0|         0|          0|          0|   0|     1|          1|         0|   0|                0|              0|           0|            0|         0|             1|                0|  0|        0|         0|             0|       0|        0|        0|  0|    0|    0|         0|   0|   0|        0|      0|           0|   0|      0|          0|       0|           0|          0|    0|            0|               0|    0|               0|      0|        0|           0|            0|            0|        0|       0|   0|       0|     0|           0|      0|        0|     0|           0|   0|   0|         0|     0|      0|               0|    0|         0|   0|             0|    0|           0|   0|        0|      0|   0|     0|\n",
            "|           0|        0|      0|             0|     0|        0|   0|      0|      0|           0|                0|     0|     0|    0|        0|              0|    0|       0|     0|            0|                 0|          0|      0|          1|             0|       0|         0|          0|          0|   0|     0|          0|         0|   0|                0|              0|           0|            0|         0|             0|                0|  0|        1|         0|             0|       0|        0|        0|  0|    0|    0|         0|   1|   0|        0|      0|           0|   0|      0|          0|       0|           0|          0|    0|            0|               0|    0|               0|      0|        1|           0|            0|            0|        0|       0|   0|       0|     0|           0|      0|        0|     0|           0|   0|   0|         0|     0|      0|               0|    0|         0|   0|             0|    0|           0|   0|        0|      0|   0|     0|\n",
            "|           0|        0|      0|             0|     0|        0|   0|      0|      0|           0|                0|     0|     1|    0|        0|              0|    0|       0|     0|            0|                 0|          0|      0|          1|             0|       0|         0|          0|          0|   1|     0|          0|         0|   0|                0|              0|           0|            0|         0|             0|                0|  0|        0|         0|             0|       0|        0|        0|  0|    0|    0|         0|   0|   0|        0|      0|           0|   0|      0|          0|       0|           0|          0|    0|            0|               0|    0|               0|      0|        0|           0|            0|            0|        1|       0|   0|       0|     0|           0|      0|        0|     0|           0|   0|   0|         0|     0|      0|               0|    0|         0|   0|             0|    0|           0|   0|        1|      0|   0|     0|\n",
            "|           0|        0|      0|             0|     0|        0|   0|      0|      0|           0|                0|     0|     0|    0|        0|              0|    0|       0|     0|            0|                 0|          0|      0|          0|             0|       0|         0|          0|          0|   0|     0|          0|         0|   0|                0|              0|           0|            0|         0|             0|                0|  0|        0|         0|             0|       0|        0|        0|  0|    0|    0|         0|   0|   0|        0|      0|           1|   0|      0|          0|       0|           0|          0|    0|            0|               0|    0|               0|      0|        0|           0|            0|            0|        0|       0|   0|       0|     0|           0|      0|        0|     0|           0|   0|   0|         0|     0|      0|               0|    0|         0|   0|             0|    0|           0|   0|        0|      0|   0|     0|\n",
            "|           0|        0|      0|             1|     0|        0|   0|      0|      0|           0|                0|     0|     0|    0|        0|              0|    0|       0|     0|            0|                 0|          0|      0|          0|             0|       0|         0|          0|          0|   0|     0|          0|         0|   0|                0|              0|           0|            1|         0|             1|                0|  0|        0|         0|             0|       0|        0|        0|  0|    0|    0|         0|   0|   0|        0|      0|           0|   0|      0|          1|       0|           0|          0|    0|            0|               0|    0|               0|      0|        0|           0|            0|            0|        0|       0|   0|       0|     0|           0|      0|        0|     0|           0|   0|   0|         0|     0|      0|               0|    0|         0|   0|             0|    0|           0|   0|        0|      1|   0|     0|\n",
            "|           0|        0|      0|             0|     0|        0|   0|      0|      0|           0|                0|     0|     0|    0|        0|              0|    0|       0|     0|            0|                 0|          0|      0|          0|             0|       0|         0|          0|          0|   0|     1|          1|         0|   0|                0|              0|           0|            0|         0|             0|                0|  0|        0|         0|             0|       0|        0|        0|  0|    0|    0|         0|   0|   0|        0|      0|           0|   0|      0|          0|       0|           0|          0|    0|            0|               0|    0|               0|      0|        0|           0|            0|            0|        0|       0|   0|       0|     0|           0|      0|        0|     0|           0|   0|   0|         0|     0|      0|               0|    0|         0|   0|             0|    0|           0|   0|        0|      1|   0|     0|\n",
            "|           0|        0|      0|             0|     0|        0|   0|      0|      0|           0|                0|     0|     1|    0|        0|              0|    0|       0|     0|            0|                 0|          0|      0|          0|             0|       0|         0|          0|          0|   0|     0|          0|         0|   0|                0|              0|           0|            0|         0|             0|                0|  0|        0|         0|             0|       0|        0|        0|  0|    0|    0|         1|   0|   0|        0|      0|           0|   0|      0|          0|       0|           0|          0|    0|            0|               0|    0|               0|      0|        0|           0|            0|            0|        0|       0|   0|       0|     0|           0|      1|        0|     0|           0|   0|   0|         0|     0|      0|               0|    0|         0|   0|             0|    0|           0|   0|        0|      0|   0|     0|\n",
            "|           0|        0|      0|             0|     0|        0|   0|      0|      0|           0|                0|     0|     0|    0|        0|              0|    0|       0|     0|            0|                 0|          0|      0|          1|             0|       0|         0|          0|          0|   0|     0|          0|         0|   1|                0|              0|           0|            0|         0|             0|                0|  0|        0|         0|             0|       0|        0|        0|  0|    0|    0|         0|   0|   0|        0|      0|           0|   0|      0|          0|       0|           0|          0|    1|            0|               0|    1|               0|      0|        0|           0|            0|            0|        0|       0|   0|       0|     1|           0|      0|        0|     0|           0|   0|   0|         0|     0|      0|               0|    0|         1|   0|             0|    1|           0|   0|        0|      0|   0|     0|\n",
            "|           0|        0|      0|             0|     0|        0|   0|      0|      0|           0|                0|     0|     0|    0|        0|              0|    0|       0|     0|            0|                 0|          0|      0|          0|             0|       0|         0|          0|          0|   0|     0|          0|         0|   0|                0|              1|           0|            0|         0|             0|                0|  0|        0|         0|             0|       0|        0|        0|  0|    0|    0|         0|   0|   0|        0|      0|           0|   0|      0|          0|       0|           0|          0|    0|            0|               0|    0|               0|      0|        0|           0|            0|            0|        0|       0|   0|       0|     0|           0|      0|        0|     0|           0|   1|   1|         0|     0|      0|               0|    0|         0|   0|             0|    0|           0|   0|        1|      0|   0|     0|\n",
            "|           0|        0|      0|             0|     0|        0|   0|      0|      0|           0|                0|     0|     0|    0|        0|              0|    0|       0|     0|            0|                 0|          0|      0|          0|             0|       0|         0|          0|          0|   0|     0|          0|         0|   0|                0|              0|           0|            0|         0|             0|                0|  0|        0|         0|             0|       0|        0|        0|  0|    0|    0|         0|   0|   0|        0|      0|           0|   0|      0|          0|       0|           0|          0|    0|            0|               0|    0|               0|      0|        0|           0|            0|            0|        0|       0|   0|       0|     0|           0|      0|        0|     0|           0|   0|   0|         0|     0|      0|               0|    0|         0|   0|             0|    0|           0|   1|        0|      0|   0|     0|\n",
            "|           0|        0|      0|             0|     0|        0|   0|      0|      0|           0|                0|     0|     0|    0|        0|              0|    0|       0|     1|            0|                 0|          0|      0|          0|             0|       0|         0|          0|          0|   0|     0|          0|         0|   0|                0|              0|           0|            0|         0|             0|                0|  0|        1|         1|             0|       0|        0|        0|  0|    0|    0|         0|   0|   0|        0|      0|           0|   0|      0|          0|       0|           0|          0|    0|            0|               0|    0|               0|      0|        0|           0|            0|            0|        0|       0|   0|       0|     0|           0|      0|        0|     0|           0|   1|   0|         0|     0|      0|               0|    0|         0|   0|             0|    0|           0|   0|        0|      0|   0|     0|\n",
            "|           0|        0|      0|             0|     0|        0|   0|      0|      0|           0|                0|     0|     0|    0|        0|              0|    0|       0|     0|            0|                 0|          0|      0|          0|             0|       0|         0|          0|          0|   0|     0|          0|         0|   0|                0|              0|           0|            0|         0|             0|                0|  0|        0|         0|             0|       0|        1|        0|  0|    0|    0|         0|   0|   0|        0|      0|           0|   0|      0|          0|       0|           0|          0|    0|            1|               0|    0|               1|      1|        0|           0|            0|            0|        0|       0|   0|       0|     0|           0|      0|        0|     0|           0|   0|   0|         0|     0|      0|               0|    0|         0|   0|             0|    0|           0|   0|        0|      0|   0|     0|\n",
            "|           0|        0|      0|             0|     0|        0|   0|      0|      0|           0|                0|     0|     0|    1|        0|              0|    0|       0|     0|            0|                 0|          0|      0|          0|             0|       0|         0|          0|          0|   0|     0|          0|         0|   0|                0|              0|           0|            0|         0|             0|                0|  0|        0|         0|             0|       0|        0|        0|  0|    0|    1|         1|   0|   0|        1|      1|           0|   0|      0|          0|       0|           0|          0|    0|            0|               1|    0|               0|      0|        0|           0|            0|            0|        0|       0|   0|       0|     0|           0|      0|        0|     0|           0|   0|   0|         0|     0|      0|               0|    0|         0|   0|             0|    0|           0|   0|        0|      0|   0|     0|\n",
            "+------------+---------+-------+--------------+------+---------+----+-------+-------+------------+-----------------+------+------+-----+---------+---------------+-----+--------+------+-------------+------------------+-----------+-------+-----------+--------------+--------+----------+-----------+-----------+----+------+-----------+----------+----+-----------------+---------------+------------+-------------+----------+--------------+-----------------+---+---------+----------+--------------+--------+---------+---------+---+-----+-----+----------+----+----+---------+-------+------------+----+-------+-----------+--------+------------+-----------+-----+-------------+----------------+-----+----------------+-------+---------+------------+-------------+-------------+---------+--------+----+--------+------+------------+-------+---------+------+------------+----+----+----------+------+-------+----------------+-----+----------+----+--------------+-----+------------+----+---------+-------+----+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Convert the given data to the format required by Spark MLlib FPGrowth."
      ],
      "metadata": {
        "id": "FhDrkKXZH_dA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "items = df.columns\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SryD0vQOuek5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2de763ed-9e28-462e-d30d-917be2223f78"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2.\tApply Spark MLlib FPGrowth to the formatted data. Mine the set of frequent patterns with the minimum support of 0.1. Mine the set of association rules with the minimum confidence of 0.9."
      ],
      "metadata": {
        "id": "OgVP1hS6IDVA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BaAIAGDeugcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise 4: Classification"
      ],
      "metadata": {
        "id": "pxSuXFXoIHO8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###0. Load the data file: mushroom.csv\n",
        "*   The data represents a collection of mushroom species.\n",
        "*   There are 8124 examples, each of which has 22 attributes and it is categorized into either “edible” (e) or “poisonous” (p)\n"
      ],
      "metadata": {
        "id": "mCMfeBzgIL0p"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SXGKh8EPIUJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.\tPrepare the train and test sets following the ratio 8:2"
      ],
      "metadata": {
        "id": "JdIdZJ2zIUge"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kAcyo8DJIZou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Fit a decision tree model on the training set, using Spark MLlib DecisionTreeClassifier with default parameters"
      ],
      "metadata": {
        "id": "zbshjQjnIZz8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TNbvDsqIIbNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Fit a random forest model on the training set, using Spark MLlib RandomForestClassification with default parameters"
      ],
      "metadata": {
        "id": "ghWaZW7LIbr-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jVhzgPw7IcOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Evaluate the two models on the same test set using the following metrics: areaUnderROC and areaUnderPR"
      ],
      "metadata": {
        "id": "TCWbqsk0IoSe"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G4YrCL5-IohK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5. Chain the above steps into a single pipeline"
      ],
      "metadata": {
        "id": "cTJAVAGeIr3a"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I3wOtZdWIzsF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 5: Clustering"
      ],
      "metadata": {
        "id": "dn-fgVNlIxZN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nqGZvNcWIxih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.\tCluster the data by using Spark MLlib KMeans with k = 2, 3, and 5, using Euclidean distance and cosine distance"
      ],
      "metadata": {
        "id": "IdHBhBHqIxqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DJV4mX3_IxyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Evaluate each of the above clustering results using silhoutte score. Which configuration yeilds the best clustering?"
      ],
      "metadata": {
        "id": "dBo8pvyOIx5p"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3NekP_ioIyCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3. Chain the above steps into a single pipeline"
      ],
      "metadata": {
        "id": "GMAMYR1mJDk8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "429A9QeCIsT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4. For each clustering result obtained above, count the number of examples that belong to each of the three species."
      ],
      "metadata": {
        "id": "h1ByWgvaJHp-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AYyWW4ltJII8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 6: Network manipulation with Spark GraphFrames"
      ],
      "metadata": {
        "id": "T5epMoIlJOr1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install graphframes"
      ],
      "metadata": {
        "id": "c5J0N_a33uQF",
        "outputId": "b56005ec-b456-417a-dc25-f45d2ecab331",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting graphframes\n",
            "  Downloading graphframes-0.6-py2.py3-none-any.whl.metadata (934 bytes)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from graphframes) (2.0.2)\n",
            "Collecting nose (from graphframes)\n",
            "  Downloading nose-1.3.7-py3-none-any.whl.metadata (1.7 kB)\n",
            "Downloading graphframes-0.6-py2.py3-none-any.whl (18 kB)\n",
            "Downloading nose-1.3.7-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.7/154.7 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nose, graphframes\n",
            "Successfully installed graphframes-0.6 nose-1.3.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###0. Load the data files: users.txt and followers.txt"
      ],
      "metadata": {
        "id": "I1pTLkFgJUmB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_df = spark.read.text(\"/content/CSC14118/users.txt\")\n",
        "user_df.show(truncate=False)"
      ],
      "metadata": {
        "id": "q1WSkukOJT2P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00e17e97-8382-40a0-c8b7-ecb9227b0368"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------------+\n",
            "|value                        |\n",
            "+-----------------------------+\n",
            "|1,BarackObama,Barack Obama   |\n",
            "|2,ladygaga,Goddess of Love   |\n",
            "|3,jeresig,John Resig         |\n",
            "|4,justinbieber,Justin Bieber |\n",
            "|6,matei_zaharia,Matei Zaharia|\n",
            "|7,odersky,Martin Odersky     |\n",
            "|8,anonsys                    |\n",
            "+-----------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "followers_df = spark.read.text(\"/content/CSC14118/followers.txt\")\n",
        "followers_df.show(truncate=False)"
      ],
      "metadata": {
        "id": "t7px8JIg4VWb",
        "outputId": "904fef2b-8f73-4b33-8588-e9979f56185c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+\n",
            "|value|\n",
            "+-----+\n",
            "|2 1  |\n",
            "|4 1  |\n",
            "|1 2  |\n",
            "|6 3  |\n",
            "|7 3  |\n",
            "|7 6  |\n",
            "|6 7  |\n",
            "|3 7  |\n",
            "+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1.\tConstruct a graph from the given data to demonstrate a tiny social network\n"
      ],
      "metadata": {
        "id": "zd6XAz-aJYlj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import split, trim\n",
        "\n",
        "edges = followers_df.select(\n",
        "    trim(split(\"value\", \" \")[0]).alias(\"src\"),\n",
        "    trim(split(\"value\", \" \")[1]).alias(\"dst\")\n",
        ")"
      ],
      "metadata": {
        "id": "y7lOQereJONE",
        "collapsed": true
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vertices = user_df.select(\n",
        "    split(\"value\", \",\").getItem(0).alias(\"id\"),\n",
        "    split(\"value\", \",\").getItem(1).alias(\"username\"),\n",
        "    split(\"value\", \",\").getItem(2).alias(\"fullname\")\n",
        ")\n"
      ],
      "metadata": {
        "id": "YSZ4HroW5xb1"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from graphframes import GraphFrame\n",
        "\n",
        "g = GraphFrame(vertices, edges)\n"
      ],
      "metadata": {
        "id": "wwvMDVxL59bJ"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "g.vertices.show()\n",
        "g.edges.show()\n",
        "\n",
        "g.inDegrees.show()\n",
        "g.outDegrees.show()\n"
      ],
      "metadata": {
        "id": "wK92f0U-6PkF",
        "outputId": "be684e61-0397-4fe6-a5ad-e2c65da4d264",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------------+---------------+\n",
            "| id|     username|       fullname|\n",
            "+---+-------------+---------------+\n",
            "|  1|  BarackObama|   Barack Obama|\n",
            "|  2|     ladygaga|Goddess of Love|\n",
            "|  3|      jeresig|     John Resig|\n",
            "|  4| justinbieber|  Justin Bieber|\n",
            "|  6|matei_zaharia|  Matei Zaharia|\n",
            "|  7|      odersky| Martin Odersky|\n",
            "|  8|      anonsys|           NULL|\n",
            "+---+-------------+---------------+\n",
            "\n",
            "+---+---+\n",
            "|src|dst|\n",
            "+---+---+\n",
            "|  2|  1|\n",
            "|  4|  1|\n",
            "|  1|  2|\n",
            "|  6|  3|\n",
            "|  7|  3|\n",
            "|  7|  6|\n",
            "|  6|  7|\n",
            "|  3|  7|\n",
            "+---+---+\n",
            "\n",
            "+---+--------+\n",
            "| id|inDegree|\n",
            "+---+--------+\n",
            "|  7|       2|\n",
            "|  3|       2|\n",
            "|  6|       1|\n",
            "|  1|       2|\n",
            "|  2|       1|\n",
            "+---+--------+\n",
            "\n",
            "+---+---------+\n",
            "| id|outDegree|\n",
            "+---+---------+\n",
            "|  7|        2|\n",
            "|  3|        1|\n",
            "|  6|        2|\n",
            "|  1|        1|\n",
            "|  4|        1|\n",
            "|  2|        1|\n",
            "+---+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2.\tApply Graphs graphPageRank to the network to obtain a ranking list of users in terms of followers"
      ],
      "metadata": {
        "id": "hSV29hkuJct7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `ranks.vertices.select(...)`\n",
        "\n",
        "  - Sau khi chạy PageRank:\n",
        "\n",
        "    - id: Id của user\n",
        "\n",
        "    - username, fullname: Thông tin về user\n",
        "\n",
        "    - pagerank: Điểm PageRank của node đó trong mạng.\n",
        "\n",
        "  - .orderBy(\"pagerank\", ascending=False)\n",
        "    \n",
        "    - Sắp xếp kết quả để hiển thị user có PageRank cao nhất lên trên.\n",
        "\n",
        "  - .show(truncate=False)\n",
        "\n",
        "    - Hiển thị toàn bộ tên (tránh cắt)"
      ],
      "metadata": {
        "id": "tBulm-7265Vr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run PageRank until convergence (tolerance 0.01)\n",
        "ranks = g.pageRank(resetProbability=0.15, tol=0.01)\n",
        "\n",
        "# Show the top users by PageRank score\n",
        "print(\"User ranking by PageRank (influence based on followers):\")\n",
        "ranks.vertices.select(\"id\", \"username\", \"fullname\", \"pagerank\") \\\n",
        "      .orderBy(\"pagerank\", ascending=False) \\\n",
        "      .show(truncate=False)"
      ],
      "metadata": {
        "id": "SNAmL2y3Jc3E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "446738ba-477e-4616-b94f-f165097080b2"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User ranking by PageRank (influence based on followers):\n",
            "+---+-------------+---------------+-------------------+\n",
            "|id |username     |fullname       |pagerank           |\n",
            "+---+-------------+---------------+-------------------+\n",
            "|1  |BarackObama  |Barack Obama   |1.6799960991181582 |\n",
            "|2  |ladygaga     |Goddess of Love|1.597343003059732  |\n",
            "|7  |odersky      |Martin Odersky |1.4475660109319795 |\n",
            "|3  |jeresig      |John Resig     |1.1216981830374868 |\n",
            "|6  |matei_zaharia|Matei Zaharia  |0.794609271048275  |\n",
            "|8  |anonsys      |NULL           |0.17939371640218368|\n",
            "|4  |justinbieber |Justin Bieber  |0.17939371640218368|\n",
            "+---+-------------+---------------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3. Find connected components on the graph, using Graphs connectedComponents or stronglyConnectedComponents"
      ],
      "metadata": {
        "id": "nIciqOCqJeqH"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TqThg_PxJe0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Upload df to MongoDB"
      ],
      "metadata": {
        "id": "rzXL8J8xsZmB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "df = spark.createDataFrame([\n",
        "    Row(name=\"Alice\", age=25),\n",
        "    Row(name=\"Bob\", age=30)\n",
        "])\n",
        "\n",
        "df.write.format(\"mongo\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .option(\"database\", \"Ming3993\") \\\n",
        "    .option(\"collection\", \"second\") \\\n",
        "    .option(\"spark.mongodb.output.uri\", mongo_uri) \\\n",
        "    .save()"
      ],
      "metadata": {
        "id": "X88gwkHWsZCB"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_read = spark.read \\\n",
        "    .format(\"mongo\") \\\n",
        "    .option(\"database\", \"Ming3993\") \\\n",
        "    .option(\"collection\", \"first\") \\\n",
        "    .option(\"spark.mongodb.input.uri\", mongo_uri) \\\n",
        "    .load()\n"
      ],
      "metadata": {
        "id": "zcuIFQ2azACQ"
      },
      "execution_count": 66,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}